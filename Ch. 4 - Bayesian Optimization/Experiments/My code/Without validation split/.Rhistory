if (which_metrics == 'test'){
if (metric == 'acc'){y=test_acc}
if (metric == 'sens'){y=test_sens}
if (metric == 'spec'){y=test_spec}
if (metric == 'f1'){y=test_f1}
} else if (which_metrics == 'validation') {
if (metric == 'acc'){y=val_acc}
if (metric == 'sens'){y=val_sens}
if (metric == 'spec'){y=val_spec}
if (metric == 'f1'){y=val_f1}
}
anova_model_test_accuracy = lm(y ~ n + p + n:p)
summary(anova_model_test_accuracy)
anova_model_test_accuracy$coefficients
#a numerical regression? Define a beta regression for this... test accuracy <= 1
install.packages("betareg")
library("betareg")
n = models_results$sample_size
p = models_results$proportion
#fits the model: test_accuracy = constant + a1*sample_size + a2*proportion + a3*interaction
regression_model_test = betareg(y ~ n + p + n:p)
summary(regression_model_test)
install.packages("betareg")
install.packages("betareg")
install.packages("betareg")
library("betareg")
n = models_results$sample_size
p = models_results$proportion
regression_model_test = betareg(y ~ n + p + n:p)
summary(regression_model_test)
regression_model = betareg(y ~ n + p + n:p)
summary(regression_model)
plot(regression_model, which = 1:4, type = "pearson")
dev.off()
plot(regression_model, which = 1:4, type = "pearson")
plot(regression_model, which = 5, type = "deviance", sub.caption = "")
models_results = read.csv('formatted_results.csv', header=T)
attach(models_results)
names(models_results)
sample_sizes = c(500, 750, 1000, 1250)
proportions = c(0.01, seq(0.05, 0.95, 0.05), 0.99)
length(proportions)
n = sample_size
p = proportion
#build a linear model to predict ... test accuracy?
#first we categorize our data.
models_results$sample_size_token = ifelse(n==500, 'n1',
ifelse(n==750, 'n2',
ifelse(n==1000, 'n3',
ifelse(n==1500, 'n4',''))))
#test y
test_sens = models_results$Test_Sensitivity
test_acc = models_results$Test_Accuracy
test_spec = models_results$Test_Specificity
test_f1 = models_results$Test_F1
#validation y
val_sens = models_results$Valid_Sensitivity
val_acc = models_results$Valid_Accuracy
val_spec = models_results$Valid_Specificity
val_f1 = models_results$Valid_F1
# Now decide what model to fit. All metrics take values between 0 and 1, so the regresion
# needs to use a link function (such as in beta regression) to ensure output between 0-1.
# For this reason, typical linear regression is inappropriate.
#Choose one of:
which_metrics = 'test' # otherwise 'validation'
metric = 'acc' # or 'sens' , 'spec', 'f1'
if (which_metrics == 'test'){
if (metric == 'acc'){y=test_acc}
if (metric == 'sens'){y=test_sens}
if (metric == 'spec'){y=test_spec}
if (metric == 'f1'){y=test_f1}
} else if (which_metrics == 'validation') {
if (metric == 'acc'){y=val_acc}
if (metric == 'sens'){y=val_sens}
if (metric == 'spec'){y=val_spec}
if (metric == 'f1'){y=val_f1}
}
anova_model_test_accuracy = lm(y ~ n + p + n:p)
summary(anova_model_test_accuracy)
anova_model_test_accuracy$coefficients
#a numerical regression? Define a beta regression for this... test accuracy <= 1
install.packages("betareg")
library("betareg")
n = models_results$sample_size
p = models_results$proportion
#fits the model using standard logit link function
regression_model = betareg(y ~ n + p + n:p)
summary(regression_model)
#model diagnostics
#plots look strange for test accuracy
plot(regression_model, which = 1:4, type = "pearson")
summary(regression_model)
samples_taken = seq(10,30,5)
samples_taken
samples_taken = seq(10,30,5)
avg_epistemic = c(0.00201,
0.00380,
0.00075,
0.00095,
0.00176)
avg_aleatoric = c(0.00464,
0.00652,
0.00292,
0.00375,
0.00377)
plot(samples_taken, avg_aleatoric)
plot(samples_taken, avg_aleatoric, lines=TRUE)
?plot
df = data.frame(
x = samples_taken,
avg_epistemic = avg_epistemic,
avg_aleatoric = avg_aleatoric)
df
df = data.frame(
samples_taken = samples_taken,
avg_epistemic = avg_epistemic,
avg_aleatoric = avg_aleatoric)
library(ggplot2)
ggplot(df, aes(samples_taken)) +
geom_line(aes(y = avg_epistemic, colour = "avg_epistemic")) +
geom_line(aes(y = avg_aleatoric, colour = "avg_aleatoric"))
ggplot(df, aes(samples_taken)) +
geom_line(aes(y = avg_epistemic, colour = "avg_epistemic")) +
geom_line(aes(y = avg_aleatoric, colour = "avg_aleatoric")) +
labs(title="Uncertainties", x ="Number of samples from var. posterior", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
ggplot(df, aes(samples_taken)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Number of samples from var. posterior", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
threshold = seq(0.1, 0.6, 0.1)
threshold = seq(0.1, 0.6, 0.1)
threshold
threshold = seq(0.1, 0.6, 0.1)
avg_epistemic = c(0.00110,
0.00142,
0.00209,
0.00095,
0.00057)
avg_aleatoric = c(0.00264,
0.00466,
0.00536,
0.00287,
0.00221)
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
threshold = seq(0.1, 0.6, 0.1)
avg_epistemic = c(0.00110,
0.00142,
0.00209,
0.00095,
0.00057,
0.00097)
avg_aleatoric = c(0.00264,
0.00466,
0.00536,
0.00287,
0.00221,
0.00336)
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
threshold = seq(0.1, 0.6, 0.1)
avg_epistemic = c(0.00110,
0.00142,
0.00209,
0.00095,
0.00057,
0.00097)
avg_aleatoric = c(0.00264,
0.00466,
0.00536,
0.00287,
0.00221,
0.00336)
df = data.frame(
threshold = threshold,
avg_epistemic = avg_epistemic,
avg_aleatoric = avg_aleatoric)
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = threshold[seq(1, length(threshold), 2)])
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = threshold[seq(1, length(threshold), 1)])
samples_taken = seq(10,30,5)
avg_epistemic = c(0.00201,
0.00380,
0.00075,
0.00095,
0.00176)
avg_aleatoric = c(0.00464,
0.00652,
0.00292,
0.00375,
0.00377)
df = data.frame(
samples_taken = samples_taken,
avg_epistemic = avg_epistemic,
avg_aleatoric = avg_aleatoric)
ggplot(df, aes(samples_taken)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties", x ="Number of samples from var. posterior", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5))
threshold = seq(0.1, 0.6, 0.1)
avg_epistemic = c(0.00110,
0.00142,
0.00209,
0.00095,
0.00057,
0.00097)
avg_aleatoric = c(0.00264,
0.00466,
0.00536,
0.00287,
0.00221,
0.00336)
df = data.frame(
threshold = threshold,
avg_epistemic = avg_epistemic,
avg_aleatoric = avg_aleatoric)
ggplot(df, aes(threshold)) +
geom_line(aes(y = avg_epistemic, colour = "Average epistemic uncertainty")) +
geom_line(aes(y = avg_aleatoric, colour = "Average aleatoric uncertainty")) +
labs(title="Uncertainties when network can refuse", x ="Threshold", y = "Avg. Uncertainties") + theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = threshold[seq(1, length(threshold), 1)])
data = read.csv('BayesOpt_Hyperparameters_keras_MNIST.csv')
data = read.csv('C:\Users\John\Desktop\Thesis\Ch. 4 - Bayesian Optimization\Experiments\My code\BayesOpt_Hyperparameters_keras_MNIST.csv')
data = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST.csv')
data
shape(data)
head(data)
hyperparams = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST.csv')
attach(hyperparams)
l1_drop = Dropout.in.layer.1
l2_drop = Dropout.in.layer.2
l1_out = Layer.1.size
l2_out = Layer.2.size
batch_size = Batch.size
#epochs =  Epochs
validation_split = Validation.split
loss = Loss
acc = Accuracy
loss_model = lm(loss ~ l1_drop + l2_drop + l1_out + l2_out + batch_size + validation_split)
summary(loss_model)
acc_model = lm(acc ~ l1_drop + l2_drop + l1_out + l2_out + batch_size + validation_split)
summary(acc_model)
l1_drop = scale(Dropout.in.layer.1)
l1_drop = scale(Dropout.in.layer.1)
l2_drop = scale(Dropout.in.layer.2)
l1_out = scale(Layer.1.size)
l2_out = scale(Layer.2.size)
batch_size = scale(Batch.size)
epochs =  scale(Epochs)
validation_split = scale(Validation.split)
loss = Loss
acc = Accuracy
yperparams = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST.csv')
attach(hyperparams)
l1_drop = scale(Dropout.in.layer.1)
l2_drop = scale(Dropout.in.layer.2)
l1_out = scale(Layer.1.size)
l2_out = scale(Layer.2.size)
batch_size = scale(Batch.size)
epochs =  scale(Epochs)
validation_split = scale(Validation.split)
loss = Loss
acc = Accuracy
hyperparams = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST.csv')
epochs =  scale(Epochs)
epochs =  scale(Epoch)
names(hyperparams)
attach(hyperparams)
hyperparams = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST.csv')
attach(hyperparams)
l1_drop = scale(Dropout.in.layer.1)
l2_drop = scale(Dropout.in.layer.2)
l1_out = scale(Layer.1.size)
l2_out = scale(Layer.2.size)
batch_size = scale(Batch.size)
epochs =  scale(Epochs)
validation_split = scale(Validation.split)
loss = Loss
acc = Accuracy
loss_m = loss ~ l1_drop + l2_drop + l1_out + l2_out + batch_size + validation_split
loss_model = lm(loss_m)
summary(loss_model)
acc_m = acc ~ l1_drop + l2_drop + l1_out + l2_out + batch_size + validation_split
acc_model = lm(acc_model)
summary(acc_model)
resid(acc_model)
plot(resid(acc_model))
data(dja)
install.packages("aods3")
library("aods3")
data(dja)
dja
attach(dja)
df = dja
df <- df[ -c(trisk) ]
df
df = dja
df <- df[ -c(5) ]
df
?dja
?rBB
install.packages("HRQoL")
library("HRQoL")
?rBB
attach(df)
names(df)
names(df)
?dja
df <- df[ -c(2,5) ]
df
df = dja
head(df)
df <- df[ -c(2,3,5) ]
head(df)
attach(df)
model_to_fit = m ~ group + n
?BBreg
model <- BBreg(model_to_fit)
max_score = 10
model <- BBreg(model_to_fit, m = )
model <- BBreg(model_to_fit, m = max_score)
model_to_fit = m ~ group + n
model_to_fit = df$m ~ group + n
max_score = 10
model <- BBreg(model_to_fit, m = max_score)
df['m'] = 'mortality'
df
df = dja
df <- df[ -c(2,3,5) ]
head(df)
names(df) = 'mortality'
head(df)
df = dja
df <- df[ -c(2,3,5) ]
head(df)
attach(df)
names(df)
names[2](df) = 'mortality'
names(df)[2] = 'mortality'
head(df)
df = dja
df <- df[ -c(2,3,5) ]
head(df)
attach(df)
names(df)
names(df)[3] = 'mortality'
head(df)
model_to_fit = mortality ~ group + n
max_score = 10
model <- BBreg(model_to_fit, m = max_score)
df = dja
df <- df[ -c(2,3,5) ]
head(df)
attach(df)
names(df)
names(df)[3] = 'mortalities'
head(df)
set.seed(18)
beta <- c(-10,2) #random values
p <- 1/(1+exp(-(beta[1]+beta[2]*x))) #change these up
phi <- 1.2 #another parameter of the beta-binomial model
y <- rBB(k,m,p,phi)
model_to_fit = mortalities ~ group + n
max_score = 10
model <- BBreg(model_to_fit, m = max_score)
attach(df)
head(df)
model_to_fit = mortalities ~ group + n
max_score = 10
model <- BBreg(model_to_fit, m = max_score)
length(n)
length(mortalities)
df = dja
df <- df[ -c(2,3,5) ]
head(df)
attach(df)
names(df)
names(df)[3] = 'mortalities'
head(df)
df = dja
head(df)
attach(df)
df <- df[ -c(2,3,5) ]
names(df)
names(df)[2] = 'exposures'
names(df)[3] = 'mortalities'
attach(df)
head(df)
model_to_fit = mortalities ~ group + exposures
max_score = 10
model <- BBreg(model_to_fit, m = max_score)
max_score = 100
model <- BBreg(model_to_fit, m = max_score)
model
getwd()
hyperparams_EI = read.csv('BayesOpt_Hyperparameters_keras_MNIST_EI.csv')
setwd('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//')
hyperparams_EI = read.csv('BayesOpt_Hyperparameters_keras_MNIST_EI.csv')
setwd('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//Without validation split//')
hyperparams_EI = read.csv('BayesOpt_Hyperparameters_keras_MNIST_EI.csv')
hyperparams_MPI = read.csv('BayesOpt_Hyperparameters_keras_MNIST_MPI.csv')
hyperparams_LCB = read.csv('C://Users//John//Desktop//Thesis//Ch. 4 - Bayesian Optimization//Experiments//My code//BayesOpt_Hyperparameters_keras_MNIST_LCB.csv')
hyperparams_LCB = read.csv('BayesOpt_Hyperparameters_keras_MNIST_LCB.csv')
attach(hyperparams_EI)
l1_drop = Dropout.in.layer.1
l2_drop = Dropout.in.layer.2
l1_out = Layer.1.size
l2_out = Layer.2.size
batch_size = Batch.size
epochs =  Epochs
EI_loss = Loss
EI_acc = Accuracy
attach(hyperparams_MPI)
l1_drop = Dropout.in.layer.1
l2_drop = Dropout.in.layer.2
l1_out = Layer.1.size
l2_out = Layer.2.size
batch_size = Batch.size
epochs =  Epochs
MPI_loss = Loss
MPI_acc = Accuracy
EI_loss
MPI_loss
# LCB:
attach(hyperparams_LCB)
l1_drop = Dropout.in.layer.1
l2_drop = Dropout.in.layer.2
l1_out = Layer.1.size
l2_out = Layer.2.size
batch_size = Batch.size
epochs =  Epochs
LCB_loss = Loss
LCB_acc = Accuracy
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy', xaxt='n', xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy', xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set accuracy',ylim=c(0.955,1), xlab = 'Number of BayesOpt iterations')
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set accuracy', xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy', xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy',ylim=c(0.955,1), xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy',ylim=c(0.96,0.99), xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set loss', xlab = 'Number of BayesOpt iterations')
lines(MPI_loss, col = 'green')
lines(LCB_loss, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set loss',ylim=c(0.05,0.11), xlab = 'Number of BayesOpt iterations')
lines(MPI_loss, col = 'green')
lines(LCB_loss, col = 'blue')
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set loss',ylim=c(0.05,0.12), xlab = 'Number of BayesOpt iterations')
lines(MPI_loss, col = 'green')
lines(LCB_loss, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_acc, col='red', cex = 0.5,main='Network test-set accuracy based on choice of acquisition',
type='l', ylab='Test-set accuracy',ylim=c(0.96,0.99), xlab = 'Number of BayesOpt iterations')
lines(MPI_acc, col = 'green')
lines(LCB_acc, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
plot(EI_loss, col='red', cex = 0.5,main='Network test-set loss based on choice of acquisition',
type='l', ylab='Test-set loss',ylim=c(0.05,0.12), xlab = 'Number of BayesOpt iterations')
lines(MPI_loss, col = 'green')
lines(LCB_loss, col = 'blue')
legend('bottomright', legend=c("EI", "MPI", "LCB"),
col=c("red", "green", "blue"), lty=1:2, cex=0.8)
